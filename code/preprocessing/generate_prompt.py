import ollama
import pandas as pd
import google.generativeai as genai
import os

# Note that Google Gemini offers $300 of free credits (without up front charges)
# Or you can simply use gemini 1.5-flash for free
os.getenv('GOOGLE_API_KEY')
genai.configure(api_key='your_key_here')

def get_question(prompt):
  '''
  **This function takes a prompt and prompt an answer from the LOCAL LLM**
  Args:
  prompt (str) = prompt to ask the LLM
  Returns:
  the response from the LLM
  '''
  response = ollama.chat(model = 'your_model_name', messages = [
    {
      'role': 'user',
      'content': prompt,
    },
  ])
    
  return response['message']['content']

def get_question_api(prompt):
  '''
  **This function takes a prompt and prompt an answer the LLM using API inference**
  Args:
  prompt (str) = prompt to ask the LLM
  Returns:
  the response from the LLM
  '''
  try:
    model = genai.GenerativeModel(model_name="your_model_name")
    # Since Dostoevsky's text contains explicit content to a certain degree, it is neccessary to turn off the blockers
    response = model.generate_content(prompt, safety_settings=[
      {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
      {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
      {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
      {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    ])
    return response.text
  except Exception as e:
    print(f"An error occurred: {e}")
    # Return a default value (e.g., empty string) if there's an error
    return ""

def get_answer(prompt):
  '''
  **This function takes a prompt and prompt an answer from the local LLM**
  Args:
  prompt (str) = prompt to ask the LLM
  Returns:
  the response from the LLM
  '''
  response = ollama.chat(model = 'your_model_name', messages = [
    {
      'role': 'user',
      'content': prompt,
    },
  ])
    
  return response['message']['content']

def get_answer_api(prompt):
  '''
  **This function takes a prompt and prompt an answer the LLM using API inference**
  Args:
  prompt (str) = prompt to ask the LLM
  Returns:
  the response from the LLM
  '''
  try:
    model = genai.GenerativeModel(model_name="your_model_name")
    # Since Dostoevsky's text contains explicit content to a certain degree, it is neccessary to turn off the blockers
    response = model.generate_content(prompt, safety_settings=[
      {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
      {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
      {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
      {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    ])
    return response.text
  except Exception as e:
    print(f"An error occurred: {e}")
    # Return a default value (e.g., empty string) if there's an error
    return ""

def generate_prompts(text):
  '''
  **This function uses a chunk of text to prompt the LLM for a question and an answer**
  Args:
  text (str) = the chunk of text we want LLM to infer about
  Returns:
  a question and an answer pair generated by the LLM
  '''
  question_prompt = f"You are a man who gets to meet Dostoevsky in person, and you are deeply intrigued of his ideas. Using the provided context: '{text}', generate a single question that deeply captures an idea or theme presented in the context, do not mention a character or an event in the context, just an idea it portrays. The question should be thought-provoking and focused on extracting a crucial aspect from the text."
  question = get_question(question_prompt)

  answer_prompt = f"You are Fyodor Dostoevsky, incorporating his literary tone as well, an old friend has asked you to give a detailed answer (maximum is 300 words) to the question: '{question}', given the context: '{text}'. Answer in first person view, as if you are Dostoevsky himself, give no editorial comments and references to external sources."
  answer = get_answer(answer_prompt)

  return question, answer

def generate_prompts_api(text):
  '''
  **This function uses a chunk of text to prompt the LLM for a question and an answer**
  Args:
  text (str) = the chunk of text we want LLM to infer about
  Returns:
  a question and an answer pair generated by the LLM
  '''
  question_prompt = f"You are an insightful, young scholar deeply intrigued by Dostoevskyâ€™s exploration of existential themes. Using the provided context: '{text}', generate a single question that deeply captures a key notion, theme, or significant insight from the context. The question should be thought-provoking and focused on extracting a crucial aspect from the text."
  question = get_question_api(question_prompt)

  answer_prompt = f"You are Fyodor Dostoevsky, incorporating his literary tone as well, someone has asked you to give a detailed answer (in less than 350 words) to the question: '{question}', given the context: '{text}'. Answer in first person view, as if you are Dostoevsky himself, give no editorial comments and references to external sources."
  answer = get_answer_api(answer_prompt)

  return question, answer

def append_to_csv(input_data, output_data, csv_file):
  '''
  **This function takes 2 data and save it in a row in the CSV**
  Args:
  input_data (str) = a question generated by the LLM
  output_data (str) = a corresponding answer generated by the LLM
  '''
  #Create a DataFrame from the new data
  new_data = {'input': [input_data], 'output': [output_data]}
  df = pd.DataFrame(new_data)

  #Append to the existing CSV without header
  df.to_csv(csv_file, mode='a', header = False, index = False)
